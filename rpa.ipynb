{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "335eeee7",
   "metadata": {},
   "source": [
    "# RPA - Sincronização de Banco de Dados SaveIt (v2.0)\n",
    "\n",
    "Este notebook sincroniza dados entre dois bancos (primeira e segunda série), mapeando corretamente os IDs gerados automaticamente e atualizando as chaves estrangeiras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cefce837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6b8556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c541e51",
   "metadata": {},
   "source": [
    "## Função de Upsert com Mapeamento de IDs (v2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c9613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_composite_key(df, columns):\n",
    "    # Concatena os valores das colunas com um separador\n",
    "    composite = df[columns].astype(str).agg('||'.join, axis=1)\n",
    "    return composite\n",
    "\n",
    "\n",
    "def upsert_data_with_fks(df, unique_col, columns_list, table, cur, conn, pk_col=\"id\", fk_mappings=None, composite_key_cols=None):\n",
    "    # Cria uma cópia do DataFrame para não modificar o original\n",
    "    df_work = df.copy()\n",
    "    \n",
    "    # Guarda os IDs originais\n",
    "    df_work[\"_old_id\"] = df_work[pk_col].copy()\n",
    "    \n",
    "    # Se usar chave composta, cria uma coluna temporária\n",
    "    if composite_key_cols:\n",
    "        df_work[\"_composite_key\"] = create_composite_key(df_work, composite_key_cols)\n",
    "        unique_col = \"_composite_key\"\n",
    "        # Adiciona a coluna composta à lista de colunas a inserir temporariamente\n",
    "        columns_list_with_key = columns_list.copy()\n",
    "    else:\n",
    "        columns_list_with_key = columns_list\n",
    "    \n",
    "    # Atualiza as chaves estrangeiras se fornecidas\n",
    "    if fk_mappings:\n",
    "        for fk_col, mapping_dict in fk_mappings:\n",
    "            if fk_col in df_work.columns:\n",
    "                df_work[fk_col] = df_work[fk_col].map(mapping_dict)\n",
    "                # Verifica se há valores não mapeados\n",
    "                unmapped = df_work[df_work[fk_col].isna()][fk_col]\n",
    "                if len(unmapped) > 0:\n",
    "                    print(f\"AVISO: {len(unmapped)} valores não mapeados em {fk_col}\")\n",
    "    \n",
    "    # Remove a coluna de ID para inserção\n",
    "    df_insert = df_work.drop(columns=[pk_col])\n",
    "    \n",
    "    cols = \", \".join(columns_list)\n",
    "    placeholders = \", \".join([\"%s\"] * len(columns_list))\n",
    "    \n",
    "    # Se não houver coluna única, insere sem ON CONFLICT (Busca + Insere)\n",
    "    if composite_key_cols:\n",
    "        inserted_rows = []\n",
    "        \n",
    "        for idx, row in df_insert.iterrows():\n",
    "            values = [row[col] for col in columns_list]\n",
    "            \n",
    "            # Primeiro tenta encontrar se já existe\n",
    "            where_conditions = \" AND \".join([f\"{col} = %s\" for col in columns_list])\n",
    "            check_query = f\"SELECT {pk_col} FROM {table} WHERE {where_conditions};\"\n",
    "            \n",
    "            try:\n",
    "                cur.execute(check_query, values)\n",
    "                existing = cur.fetchone()\n",
    "                \n",
    "                if existing:\n",
    "                    # Já existe, usa o ID existente\n",
    "                    new_id = existing[0]\n",
    "                    composite_key = row[\"_composite_key\"]\n",
    "                    inserted_rows.append((new_id, composite_key))\n",
    "                    # print(f\"  → Registro já existe (ID={new_id})\")\n",
    "                else:\n",
    "                    # Não existe, insere novo\n",
    "                    insert_query = f\"\"\"\n",
    "                    INSERT INTO {table} ({cols})\n",
    "                    VALUES ({placeholders})\n",
    "                    RETURNING {pk_col};\n",
    "                    \"\"\"\n",
    "                    cur.execute(insert_query, values)\n",
    "                    result = cur.fetchone()\n",
    "                    conn.commit()\n",
    "                    \n",
    "                    if result:\n",
    "                        new_id = result[0]\n",
    "                        composite_key = row[\"_composite_key\"]\n",
    "                        inserted_rows.append((new_id, composite_key))\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar registro na tabela {table}: {e}\")\n",
    "                print(f\"Valores: {values}\")\n",
    "                conn.rollback()\n",
    "    else:\n",
    "        # Estratégia original com ON CONFLICT\n",
    "        excluded = \", \".join([f\"{col} = EXCLUDED.{col}\" for col in columns_list if col != unique_col])\n",
    "        inserted_rows = []\n",
    "        \n",
    "        for idx, row in df_insert.iterrows():\n",
    "            values = [row[col] for col in columns_list]\n",
    "            query = f\"\"\"\n",
    "            INSERT INTO {table} ({cols})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT ({unique_col}) DO UPDATE\n",
    "            SET {excluded}\n",
    "            RETURNING {pk_col}, {unique_col};\n",
    "            \"\"\"\n",
    "            try:\n",
    "                cur.execute(query, values)\n",
    "                result = cur.fetchone()\n",
    "                conn.commit()\n",
    "                if result:\n",
    "                    inserted_rows.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao inserir na tabela {table}: {e}\")\n",
    "                print(f\"Valores: {values}\")\n",
    "                conn.rollback()\n",
    "    \n",
    "    # Cria DataFrame com os IDs gerados\n",
    "    df_ids = pd.DataFrame(inserted_rows, columns=[pk_col, unique_col])\n",
    "    \n",
    "    # Cria mapping old_id -> new_id\n",
    "    mapping = df_work.merge(df_ids, on=unique_col, how=\"left\", suffixes=('_old', '_new'))\n",
    "    mapping_dict = dict(zip(mapping[\"_old_id\"], mapping[f\"{pk_col}_new\"]))\n",
    "        \n",
    "    return mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe17149",
   "metadata": {},
   "source": [
    "## Conexão com os Bancos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab76d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Conexões estabelecidas com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Estabelecendo conexão com os databases\n",
    "try:\n",
    "    first_conn = psycopg2.connect(\n",
    "        getenv(\"FIRST_DATABASE_ACCESS\")\n",
    "    )\n",
    "    first_cur = first_conn.cursor()\n",
    "    \n",
    "    second_conn = psycopg2.connect(\n",
    "        getenv(\"SECOND_DATABASE_ACCESS\")\n",
    "    )\n",
    "    second_cur = second_conn.cursor()\n",
    "    print(\"✓ Conexões estabelecidas com sucesso\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar aos bancos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e13ef",
   "metadata": {},
   "source": [
    "## Extração de Dados do Banco de Origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f09112e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Extraindo dados do banco de origem ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabrielbento-ieg\\AppData\\Local\\Temp\\ipykernel_14764\\4092870287.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_admin_first = pd.read_sql(\"\"\"\n",
      "C:\\Users\\gabrielbento-ieg\\AppData\\Local\\Temp\\ipykernel_14764\\4092870287.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_enterprise_first = pd.read_sql(\"\"\"\n",
      "C:\\Users\\gabrielbento-ieg\\AppData\\Local\\Temp\\ipykernel_14764\\4092870287.py:34: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_address_first = pd.read_sql(\"\"\"\n",
      "C:\\Users\\gabrielbento-ieg\\AppData\\Local\\Temp\\ipykernel_14764\\4092870287.py:49: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_employee_first = pd.read_sql(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Extraindo tabelas do banco da primeira série\n",
    "try:    \n",
    "    print(\"--- Extraindo dados do banco de origem ---\")\n",
    "    \n",
    "    df_admin_first = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            nome_admin AS name,\n",
    "            email,\n",
    "            senha AS password\n",
    "        FROM admin\n",
    "        \"\"\",\n",
    "        first_conn\n",
    "    )\n",
    "      \n",
    "    df_enterprise_first = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            c.id,\n",
    "            c.cnpj,\n",
    "            c.email,\n",
    "            c.senha AS password,\n",
    "            c.nome AS name,\n",
    "            MAX(t.num_telefone) AS phone_number,\n",
    "            c.id_endereco AS address_id,\n",
    "            COALESCE(MAX(i.id_plano), 1) AS plan_id\n",
    "        FROM cliente c\n",
    "        JOIN telefone t ON t.id_cliente = c.id\n",
    "        LEFT JOIN industria i ON i.id_cliente = c.id\n",
    "        GROUP BY c.id, c.cnpj, c.email, password, name, address_id\n",
    "        \"\"\", \n",
    "        first_conn\n",
    "    )\n",
    "\n",
    "    df_address_first = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            cep,\n",
    "            cep_cidade AS city,\n",
    "            cep_bairro AS neighbourhood,\n",
    "            cep_rua_num AS house_number,\n",
    "            cep_estado AS state,\n",
    "            cep_rua AS public_place,\n",
    "            cep_complemento AS complement\n",
    "        FROM endereco\n",
    "        \"\"\",\n",
    "        first_conn\n",
    "    )\n",
    "\n",
    "    df_employee_first = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            nome AS name,\n",
    "            email,\n",
    "            senha AS password,\n",
    "            id_empresa AS enterprise_id,\n",
    "            COALESCE(is_admin, false) AS is_admin\n",
    "        FROM funcionario\n",
    "        WHERE id_empresa is not null\n",
    "        \"\"\",\n",
    "        first_conn\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao extrair dados: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9432339",
   "metadata": {},
   "source": [
    "## Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354f8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando dados adicionais\n",
    "df_admin_first['write'] = False\n",
    "df_admin_first['image'] = None\n",
    "\n",
    "# Inserção de imagem genérica para inserção no banco da segunda série\n",
    "default_image = \"https://res.cloudinary.com/dxdjsvo0e/image/upload/v1761314229/f682575d-3083-45b2-a065-f043cf789d64_aonwbs.jpg\"\n",
    "df_enterprise_first[\"enterprise_image\"] = default_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a251829a",
   "metadata": {},
   "source": [
    "## Sincronização de Dados\n",
    "\n",
    "A ordem de inserção é importante devido às dependências de chaves estrangeiras:\n",
    "1. Admin (sem dependências)\n",
    "2. Address (sem dependências)\n",
    "3. Enterprise (depende de Address)\n",
    "4. Employee (depende de Enterprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690128ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando sincronização ---\n",
      "\n",
      "1. Inserindo admins...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Iniciando sincronização ---\")\n",
    "\n",
    "# 1. Admin insert (COM coluna única)\n",
    "print(\"\\n1. Inserindo admins...\")\n",
    "admin_mapping = upsert_data_with_fks(\n",
    "    df_admin_first, \n",
    "    'email', \n",
    "    ['name', 'email', 'password', 'write'], \n",
    "    'admin_saveit', \n",
    "    second_cur, \n",
    "    second_conn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8775ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Inserindo endereços...\n"
     ]
    }
   ],
   "source": [
    "# 2. Address insert (SEM COLUNA ÚNICA - usa chave composta)\n",
    "print(\"\\n2. Inserindo endereços...\")\n",
    "address_mapping = upsert_data_with_fks(\n",
    "    df_address_first,\n",
    "    None,  # Não há coluna única\n",
    "    ['cep', 'city', 'neighbourhood', 'house_number', 'state', 'public_place', 'complement'], \n",
    "    'address', \n",
    "    second_cur, \n",
    "    second_conn,\n",
    "    composite_key_cols=['cep', 'city', 'neighbourhood', 'house_number']  # Chave composta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47f7593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Inserindo empresas...\n"
     ]
    }
   ],
   "source": [
    "# 3. Enterprise insert (depende de address)\n",
    "print(\"\\n3. Inserindo empresas...\")\n",
    "enterprise_mapping = upsert_data_with_fks(\n",
    "    df_enterprise_first, \n",
    "    'cnpj', \n",
    "    ['cnpj', 'email', 'password', 'name', 'phone_number', 'address_id', 'plan_id', 'enterprise_image'], \n",
    "    'enterprise', \n",
    "    second_cur, \n",
    "    second_conn,\n",
    "    fk_mappings=[('address_id', address_mapping)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffe259da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Inserindo funcionários...\n",
      "AVISO: 4 valores não mapeados em enterprise_id\n",
      "Erro ao inserir na tabela employee: bigint out of range\n",
      "\n",
      "Valores: ['Mariana Oliveira', 'mariana@empresa1.com', '633687b0c0a44b85af838c859eb7a4f6e733ad604b8b27c57ab03e104aeadcdb', nan, False]\n",
      "Erro ao inserir na tabela employee: bigint out of range\n",
      "\n",
      "Valores: ['Carlos Silva', 'carlos@empresa1.com', '55a5e9e78207b4df8699d60886fa070079463547b095d1a05bc719bb4e6cd251', nan, True]\n",
      "Erro ao inserir na tabela employee: bigint out of range\n",
      "\n",
      "Valores: ['Rafael Nogueira', 'rafael.nogueira@gmaio.com', '675bf690d55e2a543a85e3c0999d0ce0e0c737e61c84d2926d92b59084b92e28', nan, False]\n",
      "Erro ao inserir na tabela employee: bigint out of range\n",
      "\n",
      "Valores: ['Beatriz Andrade', 'beatriz.andrade@gmail.com', 'd2f3dcf179934a213f2747a7dadc75b41a9fce6329b9af065cc111a71ede02e5', nan, False]\n"
     ]
    }
   ],
   "source": [
    "# 4. Employee insert (depende de enterprise)\n",
    "print(\"\\n4. Inserindo funcionários...\")\n",
    "employee_mapping = upsert_data_with_fks(\n",
    "    df_employee_first, \n",
    "    'email', \n",
    "    ['name', 'email', 'password', 'enterprise_id', 'is_admin'], \n",
    "    'employee', \n",
    "    second_cur, \n",
    "    second_conn,\n",
    "    fk_mappings=[('enterprise_id', enterprise_mapping)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6ef8a",
   "metadata": {},
   "source": [
    "## Fechamento das Conexões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92191f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conexões fechadas\n"
     ]
    }
   ],
   "source": [
    "# Fechando conexões\n",
    "first_cur.close()\n",
    "first_conn.close()\n",
    "second_cur.close()\n",
    "second_conn.close()\n",
    "print(\"\\nConexões fechadas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
